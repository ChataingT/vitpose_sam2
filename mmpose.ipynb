{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\mmengine\\optim\\optimizer\\zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import mimetypes\n",
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import json_tricks as json\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "from mmengine.logging import print_log\n",
    "\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples, split_instances\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "\n",
    "# try:\n",
    "    # from mmdet.apis import inference_detector, init_detector\n",
    "    # has_mmdet = True\n",
    "# except (ImportError, ModuleNotFoundError):\n",
    "    # has_mmdet = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(line_argument=None):\n",
    "    \"\"\"Visualize the demo images.\n",
    "\n",
    "    Using mmdet to detect the human.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    # parser.add_argument('det_config', help='Config file for detection')\n",
    "    # parser.add_argument('det_checkpoint', help='Checkpoint file for detection')\n",
    "    parser.add_argument('pose_config', help='Config file for pose')\n",
    "    parser.add_argument('pose_checkpoint', help='Checkpoint file for pose')\n",
    "    parser.add_argument(\n",
    "        '--input', type=str, default='', help='Image/Video file')\n",
    "    parser.add_argument(\n",
    "        '--show',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='whether to show img')\n",
    "    parser.add_argument(\n",
    "        '--output-root',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='root of the output img file. '\n",
    "        'Default not saving the visualization images.')\n",
    "    parser.add_argument(\n",
    "        '--save-predictions',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='whether to save predicted results')\n",
    "    parser.add_argument(\n",
    "        '--device', default='cuda:0', help='Device used for inference')\n",
    "    parser.add_argument(\n",
    "        '--det-cat-id',\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help='Category id for bounding box detection model')\n",
    "    parser.add_argument(\n",
    "        '--bbox-thr',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='Bounding box score threshold')\n",
    "    parser.add_argument(\n",
    "        '--nms-thr',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='IoU threshold for bounding box NMS')\n",
    "    parser.add_argument(\n",
    "        '--kpt-thr',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='Visualizing keypoint thresholds')\n",
    "    parser.add_argument(\n",
    "        '--draw-heatmap',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Draw heatmap predicted by the model')\n",
    "    parser.add_argument(\n",
    "        '--show-kpt-idx',\n",
    "        action='store_true',\n",
    "        default=False,\n",
    "        help='Whether to show the index of keypoints')\n",
    "    parser.add_argument(\n",
    "        '--skeleton-style',\n",
    "        default='mmpose',\n",
    "        type=str,\n",
    "        choices=['mmpose', 'openpose'],\n",
    "        help='Skeleton style selection')\n",
    "    parser.add_argument(\n",
    "        '--radius',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='Keypoint radius for visualization')\n",
    "    parser.add_argument(\n",
    "        '--thickness',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Link thickness for visualization')\n",
    "    parser.add_argument(\n",
    "        '--show-interval', type=int, default=0, help='Sleep seconds per frame')\n",
    "    parser.add_argument(\n",
    "        '--alpha', type=float, default=0.8, help='The transparency of bboxes')\n",
    "    parser.add_argument(\n",
    "        '--draw-bbox', action='store_true', help='Draw bboxes of instances')\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--bboxes', type=str, help='Path to the json file which contains the bbox')\n",
    "\n",
    "    # assert has_mmdet, 'Please install mmdet to run the demo.'\n",
    "\n",
    "    args = parser.parse_args(line_argument)\n",
    "\n",
    "    assert args.show or (args.output_root != '')\n",
    "    assert args.input != ''\n",
    "    # assert args.det_config is not None\n",
    "    # assert args.det_checkpoint is not None\n",
    "\n",
    "    output_file = None\n",
    "    if args.output_root:\n",
    "        mmengine.mkdir_or_exist(args.output_root)\n",
    "        output_file = os.path.join(args.output_root,\n",
    "                                   os.path.basename(args.input))\n",
    "        if args.input == 'webcam':\n",
    "            output_file += '.mp4'\n",
    "\n",
    "    if args.save_predictions:\n",
    "        assert args.output_root != ''\n",
    "        args.pred_save_path = f'{args.output_root}/results_' \\\n",
    "            f'{os.path.splitext(os.path.basename(args.input))[0]}.json'\n",
    "\n",
    "    # build detector\n",
    "    # detector = init_detector(\n",
    "    #     args.det_config, args.det_checkpoint, device=args.device)\n",
    "    # detector.cfg = adapt_mmdet_pipeline(detector.cfg)\n",
    "\n",
    "    with open(args.bboxes, 'r') as fd:\n",
    "        bboxes = json.load(fd)\n",
    "\n",
    "    # build pose estimator\n",
    "    pose_estimator = init_pose_estimator(\n",
    "        args.pose_config,\n",
    "        args.pose_checkpoint,\n",
    "        device=args.device,\n",
    "        cfg_options=dict(\n",
    "            model=dict(test_cfg=dict(output_heatmaps=args.draw_heatmap))))\n",
    "\n",
    "    # build visualizer\n",
    "    pose_estimator.cfg.visualizer.radius = args.radius\n",
    "    pose_estimator.cfg.visualizer.alpha = args.alpha\n",
    "    pose_estimator.cfg.visualizer.line_width = args.thickness\n",
    "    visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
    "    # the dataset_meta is loaded from the checkpoint and\n",
    "    # then pass to the model in init_pose_estimator\n",
    "    visualizer.set_dataset_meta(\n",
    "        pose_estimator.dataset_meta, skeleton_style=args.skeleton_style)\n",
    "\n",
    "    if args.input == 'webcam':\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # input_type = 'webcam'\n",
    "    else:\n",
    "        input_type = mimetypes.guess_type(args.input)[0].split('/')[0]\n",
    "\n",
    "    if input_type == 'image':\n",
    "        raise NotImplementedError\n",
    "        # inference\n",
    "        # pred_instances = process_one_image(args, args.input, bboxes,\n",
    "        #                                    pose_estimator, visualizer)\n",
    "\n",
    "        # if args.save_predictions:\n",
    "        #     pred_instances_list = split_instances(pred_instances)\n",
    "\n",
    "        # if output_file:\n",
    "        #     img_vis = visualizer.get_image()\n",
    "        #     mmcv.imwrite(mmcv.rgb2bgr(img_vis), output_file)\n",
    "\n",
    "    elif input_type in ['webcam', 'video']:\n",
    "\n",
    "        if args.input == 'webcam':\n",
    "            cap = cv2.VideoCapture(0)\n",
    "        else:\n",
    "            cap = cv2.VideoCapture(args.input)\n",
    "\n",
    "        video_writer = None\n",
    "        pred_instances_list = []\n",
    "        frame_idx = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # topdown pose estimation\n",
    "            pred_instances = process_one_image(args, frame, bboxes[str(frame_idx)],\n",
    "                                               pose_estimator, visualizer,\n",
    "                                               0.001)\n",
    "\n",
    "            if args.save_predictions:\n",
    "                # save prediction results\n",
    "                pred_instances_list.append(\n",
    "                    dict(\n",
    "                        frame_id=frame_idx,\n",
    "                        instances=split_instances(pred_instances)))\n",
    "\n",
    "            # output videos\n",
    "            if output_file:\n",
    "                frame_vis = visualizer.get_image()\n",
    "\n",
    "                if video_writer is None:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                    # the size of the image with visualization may vary\n",
    "                    # depending on the presence of heatmaps\n",
    "                    video_writer = cv2.VideoWriter(\n",
    "                        output_file,\n",
    "                        fourcc,\n",
    "                        30,  # saved fps\n",
    "                        (frame_vis.shape[1], frame_vis.shape[0]))\n",
    "\n",
    "                video_writer.write(mmcv.rgb2bgr(frame_vis))\n",
    "\n",
    "            if args.show:\n",
    "                # press ESC to exit\n",
    "                if cv2.waitKey(5) & 0xFF == 27:\n",
    "                    break\n",
    "\n",
    "                time.sleep(args.show_interval)\n",
    "            frame_idx += 1\n",
    "            \n",
    "\n",
    "        if video_writer:\n",
    "            video_writer.release()\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    else:\n",
    "        args.save_predictions = False\n",
    "        raise ValueError(\n",
    "            f'file {os.path.basename(args.input)} has invalid format.')\n",
    "\n",
    "    if args.save_predictions:\n",
    "        with open(args.pred_save_path, 'w') as f:\n",
    "            json.dump(\n",
    "                dict(\n",
    "                    meta_info=pose_estimator.dataset_meta,\n",
    "                    instance_info=pred_instances_list),\n",
    "                f,\n",
    "                indent='\\t')\n",
    "        print(f'predictions have been saved at {args.pred_save_path}')\n",
    "\n",
    "    if output_file:\n",
    "        input_type = input_type.replace('webcam', 'video')\n",
    "        print_log(\n",
    "            f'the output {input_type} has been saved at {output_file}',\n",
    "            logger='current',\n",
    "            level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_image(args,\n",
    "                      img,\n",
    "                      bboxe,\n",
    "                      pose_estimator,\n",
    "                      visualizer=None,\n",
    "                      show_interval=0):\n",
    "    \"\"\"Visualize predicted keypoints (and heatmaps) of one image.\"\"\"\n",
    "\n",
    "    # predict bbox\n",
    "    # det_result = inference_detector(detector, img)\n",
    "    # pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "    # bboxes = np.concatenate(\n",
    "        # (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "    # bboxes = bboxes[np.logical_and(pred_instance.labels == args.det_cat_id,\n",
    "                                #    pred_instance.scores > args.bbox_thr)]\n",
    "    # bboxes = bboxes[nms(bboxes, args.nms_thr), :4]\n",
    "    \n",
    "    bbox_index_mapping = {i: key for i, key in enumerate(bboxe.keys())}\n",
    "    bboxes = []\n",
    "    for key in bboxe.keys():\n",
    "        bboxes.append(bboxe[key])\n",
    "    bboxes = np.array(bboxes, dtype=np.int16)\n",
    "    # predict keypoints\n",
    "    pose_results = inference_topdown(pose_estimator, img, bboxes)\n",
    "    data_samples = merge_data_samples(pose_results)\n",
    "\n",
    "    # show the results\n",
    "    if isinstance(img, str):\n",
    "        img = mmcv.imread(img, channel_order='rgb')\n",
    "    elif isinstance(img, np.ndarray):\n",
    "        img = mmcv.bgr2rgb(img)\n",
    "\n",
    "    if visualizer is not None:\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            img,\n",
    "            data_sample=data_samples,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=args.draw_heatmap,\n",
    "            draw_bbox=args.draw_bbox,\n",
    "            show_kpt_idx=args.show_kpt_idx,\n",
    "            skeleton_style=args.skeleton_style,\n",
    "            show=args.show,\n",
    "            wait_time=show_interval,\n",
    "            kpt_thr=args.kpt_thr)\n",
    "\n",
    "    # if there is no instance detected, return None\n",
    "    return data_samples.get('pred_instances', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg=[\n",
    "    r\"C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\mmpose\\configs\\body_2d_keypoint\\topdown_heatmap\\coco\\td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py\",\n",
    "     r\"C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\weights_config\\td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192-0b8234ea_20230407.pth\",\n",
    "    \"--input\", r\"C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\data\\video\\bedroom.mp4\",\n",
    "    \"--bboxes\", r\"C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\data\\video\\bbox_dict.json\",\n",
    "    \"--output-root\", r\"C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\data\\video\\bedroom_vit\",\n",
    "    \"--device\", \"cuda\",\n",
    "    # \"--radius\", \"2\",\n",
    "    # \"--thickness\", \"1\",\n",
    "    \"--skeleton-style\", \"openpose\",\n",
    "    \"--save-predictions\",\n",
    "    \"--draw-bbox\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\weights_config\\td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192-0b8234ea_20230407.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: backbone.cls_token\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\mmengine\\runner\\checkpoint.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=map_location)\n",
      "c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\mmpose\\datasets\\datasets\\utils.py:102: UserWarning: The metainfo config file \"configs/_base_/datasets/coco_openpose.py\" does not exist. A matched config file \"c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\mmpose\\.mim\\configs\\_base_\\datasets\\coco_openpose.py\" will be used instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\mmpretrain\\models\\utils\\attention.py:594: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  x = self.scaled_dot_product_attention(q, k, v, dropout_p=attn_drop)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have been saved at C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\data\\video\\bedroom_vit/results_bedroom.json\n",
      "02/04 17:14:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - the output video has been saved at C:\\Users\\chataint\\Documents\\projet\\pose_pipeline\\segmentation\\data\\video\\bedroom_vit\\bedroom.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chataint\\AppData\\Local\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\json_tricks\\encoders.py:419: UserWarning: json-tricks: numpy scalar serialization is experimental and may work differently in future versions\n",
      "  warnings.warn('json-tricks: numpy scalar serialization is experimental and may work differently in future versions')\n"
     ]
    }
   ],
   "source": [
    "main(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
